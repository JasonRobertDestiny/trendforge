#!/usr/bin/env python3
"""TrendForge æ¯æ—¥è‡ªåŠ¨åŒ–ç®¡çº¿ã€‚"""
from __future__ import annotations

import argparse
import asyncio
import json
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿æ­£ç¡®å¯¼å…¥
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import requests

from backend.crawlers.hackernews import HackerNewsCrawler
from backend.crawlers.reddit import RedditCrawler
from backend.crawlers.newsapi import NewsAPICrawler
from backend.crawlers.github_trending import GitHubTrendingCrawler
from backend.crawlers.mcp_trends import MCPTrendsCrawler
from backend.generators.dr_generator import DRGenerator
from backend.utils.config_loader import load_yaml_config
from backend.utils.deduplicator import Deduplicator
from backend.utils.filter import TrendingFilter
from backend.utils.storage import GitStorage

RAW_DATA_DIR = Path("data/trending")
PROCESSED_DATA_DIR = Path("data/processed")
CONTENT_DIR = Path("content/blog")
LOG_DIR = Path("logs")
DEFAULT_COMMIT_PREFIX = "feat: add"  # ä¿æŒæäº¤ä¿¡æ¯æ ¼å¼ä¸€è‡´


class TrendForgePipeline:
    """ä¸»æµç¨‹æ§åˆ¶å™¨ã€‚"""

    def __init__(self, use_mcp: bool = False, mcp_base: str | None = None) -> None:
        dr_conf = load_yaml_config("backend/config/dr_config.yaml")
        self.batch_size = dr_conf.get("batch_size", 3)
        self.max_articles = dr_conf.get("max_articles_per_run", 10)

        self.crawlers = self._init_crawlers(use_mcp, mcp_base)
        self.filter = TrendingFilter()
        self.deduplicator = Deduplicator()
        self.dr_generator = DRGenerator()
        self.storage = GitStorage()

        self._ensure_directories()

    def _ensure_directories(self) -> None:
        """ç¡®ä¿è¿è¡Œæ‰€éœ€ç›®å½•å­˜åœ¨ã€‚"""
        for folder in [RAW_DATA_DIR, PROCESSED_DATA_DIR, CONTENT_DIR, LOG_DIR]:
            folder.mkdir(parents=True, exist_ok=True)

    def _init_crawlers(self, use_mcp: bool, mcp_base: str | None) -> List:
        """æ ¹æ®å¼€å…³é€‰æ‹©çˆ¬è™«åˆ—è¡¨ã€‚"""
        base = (mcp_base or "http://localhost:3001").rstrip("/")

        if use_mcp:
            if self._check_mcp_available(base):
                print(f"   âœ“ ä½¿ç”¨ MCP Trends Hub: {base}")
                return [MCPTrendsCrawler(base_url=base)]
            print("   âš ï¸  MCP æœªå°±ç»ªï¼Œå›é€€æœ¬åœ°çˆ¬è™«")

        print("   â„¹ï¸ ä½¿ç”¨æœ¬åœ°ç‹¬ç«‹çˆ¬è™«")
        return [
            HackerNewsCrawler(),
            GitHubTrendingCrawler(),
            RedditCrawler(),
            NewsAPICrawler(),
        ]

    async def run_daily_pipeline(self) -> None:
        """å®Œæ•´æµç¨‹ï¼šæŠ“å– â†’ å»é‡ â†’ ç­›é€‰ â†’ DR ç”Ÿæˆ â†’ ä¿å­˜ â†’ Git æäº¤ã€‚"""
        start_time = datetime.now()
        print("\n" + "=" * 60)
        print(f"TrendForge Daily Pipeline - {start_time:%Y-%m-%d %H:%M:%S}")
        print("=" * 60)

        try:
            print("ğŸ“¡ Step 1: Fetching trending topics...")
            all_trending = await self._fetch_all_trending()
            print(f"   âœ“ Got {len(all_trending)} raw topics")

            self._save_raw_trending(all_trending)

            print("\nğŸ” Step 2: Deduplicating...")
            unique_trending = self.deduplicator.deduplicate(all_trending)
            print(f"   âœ“ {len(unique_trending)} unique after dedup")

            print("\nğŸ¯ Step 3: Filtering...")
            selected = self.filter.filter_trending(unique_trending)
            if not selected:
                print("   âš ï¸  No topics passed filter today")
                return

            if len(selected) > self.max_articles:
                selected = selected[: self.max_articles]

            for idx, topic in enumerate(selected, start=1):
                title_preview = topic["title"][:60]
                print(f"   {idx}. [{topic['source']}] {title_preview}")

            self._save_processed_trending(selected)

            print("\nğŸ“ Step 4: Generating articles via DR...")
            articles = await self._generate_articles(selected)
            print(f"   âœ“ Generated {len(articles)} articles")

            print("\nğŸ’¾ Step 5: Saving markdown files...")
            saved_files = self._save_articles(articles)
            print(f"   âœ“ Saved {len(saved_files)} files")

            print("\nğŸ“¤ Step 6: Git commit & push...")
            commit_msg = f"{DEFAULT_COMMIT_PREFIX} {len(articles)} new articles - {datetime.now():%Y-%m-%d}"
            self.storage.commit_and_push(commit_msg)

            duration = (datetime.now() - start_time).total_seconds()
            print("\n" + "=" * 60)
            print("âœ… Pipeline finished")
            print(f"è€—æ—¶: {duration:.1f}s, æ–‡ç« æ•°: {len(articles)}")
            print("=" * 60)
        except Exception as exc:  # pragma: no cover - è¿è¡Œæ—¶é”™è¯¯éœ€ç›´æ¥æš´éœ²
            print(f"\nâŒ Pipeline failed: {exc}")
            raise

    async def _fetch_all_trending(self) -> List[Dict]:
        """å¹¶å‘æ‰§è¡Œæ‰€æœ‰çˆ¬è™«ã€‚"""
        tasks = [crawler.fetch_trending() for crawler in self.crawlers]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        items: List[Dict] = []
        for idx, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"   âš ï¸  Crawler {idx} failed: {result}")
                continue
            items.extend(result)
        return items

    @staticmethod
    def _check_mcp_available(base_url: str) -> bool:
        """æ¢æµ‹ MCP æœåŠ¡å¥åº·æ¥å£ã€‚"""
        try:
            resp = requests.get(f"{base_url}/health", timeout=2)
            return resp.status_code == 200
        except Exception:
            return False

    async def _generate_articles(self, topics: List[Dict]) -> List[Tuple[Dict, str]]:
        """æ‰¹é‡è°ƒç”¨ DRï¼Œé™åˆ¶å¹¶å‘æ‰¹æ¬¡ã€‚"""
        articles: List[Tuple[Dict, str]] = []
        for start in range(0, len(topics), self.batch_size):
            batch = topics[start : start + self.batch_size]
            print(f"   â†’ Batch {start // self.batch_size + 1} ({len(batch)} items)")

            tasks = [self.dr_generator.generate_article(topic) for topic in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for topic, result in zip(batch, results):
                if isinstance(result, Exception):
                    print(f"   âš ï¸  Fail: {topic['title'][:50]} -> {result}")
                    continue
                articles.append((topic, result))
                print(f"   âœ“ Done: {topic['title'][:50]}")
        return articles

    def _save_articles(self, articles: List[Tuple[Dict, str]]) -> List[str]:
        """å†™å…¥ Markdown æ–‡ä»¶ã€‚"""
        saved: List[str] = []
        date_str = datetime.now().strftime("%Y-%m-%d")

        for topic, content in articles:
            slug = self._generate_slug(topic["title"])
            filename = f"{date_str}-{slug}.md"
            filepath = CONTENT_DIR / filename
            filepath.write_text(content, encoding="utf-8")
            saved.append(str(filepath))
        return saved

    @staticmethod
    def _generate_slug(title: str) -> str:
        """æ–‡ä»¶åä¸“ç”¨ slugï¼Œä¿æŒä¸ DR å†…éƒ¨ä¸€è‡´ã€‚"""
        import re

        slug = re.sub(r"[^\w\s-]", "", title.lower())
        slug = re.sub(r"[-\s]+", "-", slug)
        return slug[:50]

    def _save_raw_trending(self, items: List[Dict]) -> None:
        """ä¿å­˜åŸå§‹æŠ“å–ç»“æœï¼Œä¾¿äºåˆ†æä¸å›æº¯ã€‚"""
        date_str = datetime.now().strftime("%Y-%m-%d")
        payload = {
            "date": date_str,
            "timestamp": datetime.now().isoformat(),
            "count": len(items),
            "items": items,
        }
        (RAW_DATA_DIR / f"{date_str}.json").write_text(
            json.dumps(payload, ensure_ascii=False, indent=2, default=str),
            encoding="utf-8",
        )

    def _save_processed_trending(self, items: List[Dict]) -> None:
        """ä¿å­˜ç­›é€‰åçš„ç»“æœã€‚"""
        date_str = datetime.now().strftime("%Y-%m-%d")
        payload = {
            "date": date_str,
            "timestamp": datetime.now().isoformat(),
            "count": len(items),
            "items": items,
        }
        (PROCESSED_DATA_DIR / f"{date_str}.json").write_text(
            json.dumps(payload, ensure_ascii=False, indent=2, default=str),
            encoding="utf-8",
        )


def main() -> None:
    parser = argparse.ArgumentParser(description="TrendForge Pipeline")
    parser.add_argument("command", choices=["full", "crawl", "test"], help="é€‰æ‹©è¦æ‰§è¡Œçš„å‘½ä»¤")
    parser.add_argument("--use-mcp", action="store_true", help="å¯ç”¨ MCP Trends Hub æ•°æ®æº")
    parser.add_argument("--mcp-base", help="MCP æœåŠ¡åœ°å€ï¼Œé»˜è®¤ http://localhost:3001")
    args = parser.parse_args()

    pipeline = TrendForgePipeline(use_mcp=args.use_mcp, mcp_base=args.mcp_base)

    if args.command == "full":
        asyncio.run(pipeline.run_daily_pipeline())
    elif args.command == "crawl":
        asyncio.run(_run_crawl(pipeline))
    elif args.command == "test":
        print("æš‚æ— è‡ªåŠ¨åŒ–æµ‹è¯•ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œ crawl/full ä»¥éªŒè¯ã€‚")


def _format_preview(item: Dict) -> str:
    """æ ¼å¼åŒ–æ‰“å°é¢„è§ˆæ–‡æœ¬ã€‚"""
    title = item.get("title", "")
    return f"[{item.get('source', '')}] {title[:80]}"


async def _run_crawl(pipeline: TrendForgePipeline) -> None:
    """åªè¿è¡Œçˆ¬è™«å¹¶æ‰“å°éƒ¨åˆ†ç»“æœã€‚"""
    items = await pipeline._fetch_all_trending()
    print(f"Fetched {len(items)} items")
    for item in items[:5]:
        print("-", _format_preview(item))


if __name__ == "__main__":
    main()
