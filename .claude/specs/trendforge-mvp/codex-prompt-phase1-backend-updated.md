# TrendForge Backend Pipeline - Development Prompt for Codex (Updated)

## Project Context

You are building the backend pipeline for TrendForge, an automated content generation system using MetaGPT's Deep Research (DR) capability. The DR module generates comprehensive research articles that serve directly as blog content.

**Repository**: Empty GitLab repository at `/mnt/d/gitlab_deepwisdom/trendforge/`

## Core Workflow

```
TrendingæŠ“å– â†’ è‡ªåŠ¨ç­›é€‰ â†’ DRç”Ÿæˆæ·±åº¦æ–‡ç«  â†’ Gitä¿å­˜ â†’ è‡ªåŠ¨æ¨é€åˆ°ç½‘ç«™
```

**Key Point**: DRç”Ÿæˆçš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šå°±æ˜¯æœ€ç»ˆçš„blogå†…å®¹ï¼Œæ— éœ€é¢å¤–è½¬æ¢ã€‚

## Your Task

Build the Python backend pipeline that:
1. Fetches trending topics from multiple sources (HackerNews, Reddit, News API)
2. Automatically filters based on engagement and relevance rules
3. Uses MetaGPT DR to generate comprehensive research articles
4. Saves articles as Markdown files
5. Commits to Git (triggers website deployment)

## Technical Requirements

### 1. Project Structure

```
trendforge/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ api_config.yaml      # API keys (template)
â”‚   â”‚   â”œâ”€â”€ filter_rules.yaml    # Auto-filter configuration
â”‚   â”‚   â””â”€â”€ dr_config.yaml       # DR generation settings
â”‚   â”œâ”€â”€ crawlers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_crawler.py      # Abstract base class
â”‚   â”‚   â”œâ”€â”€ hackernews.py        # HackerNews crawler
â”‚   â”‚   â”œâ”€â”€ reddit.py            # Reddit crawler
â”‚   â”‚   â””â”€â”€ newsapi.py           # News API crawler
â”‚   â”œâ”€â”€ generators/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ dr_generator.py      # MetaGPT DR integration
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ deduplicator.py      # Remove duplicate topics
â”‚   â”‚   â”œâ”€â”€ filter.py            # Auto-filter logic
â”‚   â”‚   â””â”€â”€ storage.py           # Git operations
â”‚   â”œâ”€â”€ pipeline.py              # Main orchestrator
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ trending/                # Daily trending data
â”‚   â””â”€â”€ processed/               # Filtered trending
â””â”€â”€ content/
    â””â”€â”€ blog/                     # DR-generated articles
```

### 2. Dependencies (requirements.txt)

```txt
requests==2.31.0
beautifulsoup4==4.12.2
pyyaml==6.0.1
gitpython==3.1.40
schedule==1.2.0
python-dotenv==1.0.0
aiohttp==3.9.1
asyncio==3.4.3
metagpt  # Install via: pip install git+ssh://git@gitlab.deepwisdomai.com/pub/MetaGPT.git@dr4run
```

### 3. MetaGPT DR Integration (generators/dr_generator.py)

```python
from metagpt.environment.mgx.mgx_env import MGXEnv
from metagpt.roles.dr.research_leader import Researcher
import asyncio
from typing import Dict
from datetime import datetime
import re

class DRGenerator:
    """
    MetaGPT Deep Researchç”Ÿæˆå™¨
    ç›´æ¥ç”Ÿæˆå¯å‘å¸ƒçš„æ·±åº¦æ–‡ç« 
    """

    def __init__(self):
        self.researcher = Researcher()
        self.researcher.rc.env = MGXEnv()

    async def generate_article(self, topic: Dict) -> str:
        """
        ä½¿ç”¨DRç”Ÿæˆæ·±åº¦ç ”ç©¶æ–‡ç« 

        Input: trending topic dict
        Output: Markdownæ ¼å¼çš„å®Œæ•´æ–‡ç« 
        """
        # æ„å»ºç ”ç©¶query
        query = self._build_research_query(topic)

        print(f"  â†’ Generating DR for: {topic['title'][:50]}...")

        # è¿è¡ŒDeep Research
        await self.researcher.run(with_message=query)

        # è·å–æŠ¥å‘Šå†…å®¹
        report_content = self.researcher.state.report_info["report_content"]

        # æ·»åŠ frontmatterå¹¶æ ¼å¼åŒ–
        formatted_article = self._format_article(report_content, topic)

        return formatted_article

    def _build_research_query(self, topic: Dict) -> str:
        """
        æ„å»ºDRæŸ¥è¯¢prompt
        è®©DRç”Ÿæˆç¬¦åˆæˆ‘ä»¬éœ€æ±‚çš„æ·±åº¦æ–‡ç« 
        """
        query = f"""
æ·±åº¦ç ”ç©¶ä¸»é¢˜ï¼š{topic['title']}

ç ”ç©¶è¦æ±‚ï¼š
1. ç”Ÿæˆä¸€ç¯‡1000-1500å­—çš„æ·±åº¦åˆ†ææ–‡ç« 
2. åŒ…å«æŠ€æœ¯ç»†èŠ‚ã€è¡Œä¸šå½±å“ã€æœªæ¥å±•æœ›
3. å¼•ç”¨æƒå¨æ•°æ®æºå’Œæœ€æ–°ä¿¡æ¯
4. é€‚åˆæŠ€æœ¯å’Œè¿è¥å›¢é˜Ÿé˜…è¯»
5. ç»“æ„æ¸…æ™°ï¼Œè®ºè¿°æœ‰åŠ›

å‚è€ƒæ¥æºï¼š{topic.get('url', '')}
è¯é¢˜ç±»åˆ«ï¼š{topic.get('category', 'ç§‘æŠ€')}
"""
        return query

    def _format_article(self, report: str, topic: Dict) -> str:
        """
        æ ¼å¼åŒ–DRæŠ¥å‘Šä¸ºæ ‡å‡†Markdownæ–‡ç« 
        æ·»åŠ å¿…è¦çš„å…ƒæ•°æ®
        """
        # ç”Ÿæˆslugï¼ˆURLå‹å¥½çš„æ ‡é¢˜ï¼‰
        slug = self._generate_slug(topic['title'])

        # æå–æˆ–ç”Ÿæˆæ‘˜è¦ï¼ˆå–å‰150å­—ï¼‰
        excerpt = self._extract_excerpt(report)

        # æ„å»ºfrontmatter
        frontmatter = f"""---
title: "{topic['title']}"
date: {datetime.now().strftime('%Y-%m-%d')}
time: {datetime.now().strftime('%H:%M:%S')}
slug: {slug}
source: {topic['source']}
source_url: {topic.get('url', '')}
engagement_score: {topic['engagement_score']}
category: {topic.get('category', 'ç§‘æŠ€')}
tags: {self._extract_tags(topic)}
excerpt: "{excerpt}"
status: published
---"""

        # ç»„åˆæœ€ç»ˆæ–‡ç« 
        return f"{frontmatter}\n\n{report}"

    def _generate_slug(self, title: str) -> str:
        """ç”ŸæˆURLå‹å¥½çš„slug"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œè½¬å°å†™ï¼Œç©ºæ ¼å˜æ¨ªçº¿
        slug = re.sub(r'[^\w\s-]', '', title.lower())
        slug = re.sub(r'[-\s]+', '-', slug)
        return slug[:50]  # é™åˆ¶é•¿åº¦

    def _extract_excerpt(self, content: str) -> str:
        """æå–æ–‡ç« æ‘˜è¦"""
        # ç§»é™¤Markdownæ ‡è®°
        plain_text = re.sub(r'[#*`\[\]()]', '', content)
        # å–å‰150å­—
        return plain_text[:150].strip() + "..."

    def _extract_tags(self, topic: Dict) -> str:
        """æ ¹æ®æ ‡é¢˜å’Œç±»åˆ«ç”Ÿæˆæ ‡ç­¾"""
        tags = []

        # åŸºäºç±»åˆ«çš„æ ‡ç­¾
        if topic.get('category'):
            tags.append(topic['category'])

        # åŸºäºå…³é”®è¯çš„æ ‡ç­¾
        title_lower = topic['title'].lower()
        tech_keywords = {
            'ai': 'AI', 'gpt': 'GPT', 'openai': 'OpenAI',
            'google': 'Google', 'apple': 'Apple', 'microsoft': 'Microsoft',
            'blockchain': 'åŒºå—é“¾', 'web3': 'Web3', 'crypto': 'åŠ å¯†è´§å¸',
            'cloud': 'äº‘è®¡ç®—', 'database': 'æ•°æ®åº“', 'security': 'å®‰å…¨'
        }

        for keyword, tag in tech_keywords.items():
            if keyword in title_lower:
                tags.append(tag)

        return str(tags[:5])  # æœ€å¤š5ä¸ªæ ‡ç­¾
```

### 4. Crawler Implementation

#### HackerNews Crawler (crawlers/hackernews.py)
```python
import aiohttp
from datetime import datetime
from .base_crawler import BaseCrawler

class HackerNewsCrawler(BaseCrawler):
    """
    HackerNewsçƒ­é—¨è¯é¢˜çˆ¬è™«
    ä½¿ç”¨å®˜æ–¹Firebase APIï¼Œæ— éœ€è®¤è¯
    """

    API_BASE = "https://hacker-news.firebaseio.com/v0"

    async def fetch_trending(self) -> List[Dict]:
        """æŠ“å–HNçƒ­é—¨è¯é¢˜"""
        trending_items = []

        async with aiohttp.ClientSession() as session:
            # 1. è·å–çƒ­é—¨æ•…äº‹IDåˆ—è¡¨
            async with session.get(f"{self.API_BASE}/topstories.json") as resp:
                story_ids = await resp.json()
                # åªå–å‰30ä¸ª
                story_ids = story_ids[:30]

            # 2. å¹¶å‘è·å–æ¯ä¸ªæ•…äº‹çš„è¯¦æƒ…
            for story_id in story_ids:
                async with session.get(f"{self.API_BASE}/item/{story_id}.json") as resp:
                    story = await resp.json()

                    if story and story.get('title'):
                        item = {
                            'title': story['title'],
                            'url': story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                            'source': 'hackernews',
                            'engagement_score': story.get('score', 0) + story.get('descendants', 0) * 0.5,
                            'published_at': datetime.fromtimestamp(story.get('time', 0)),
                            'category': self._categorize(story['title']),
                            'raw_data': story
                        }
                        trending_items.append(item)

        return trending_items

    def _categorize(self, title: str) -> str:
        """æ ¹æ®æ ‡é¢˜åˆ†ç±»"""
        title_lower = title.lower()
        if any(kw in title_lower for kw in ['ai', 'gpt', 'llm', 'neural']):
            return 'AI'
        elif any(kw in title_lower for kw in ['blockchain', 'crypto', 'bitcoin']):
            return 'åŒºå—é“¾'
        elif any(kw in title_lower for kw in ['hack', 'security', 'vulnerability']):
            return 'å®‰å…¨'
        else:
            return 'ç§‘æŠ€'
```

### 5. Auto-Filter Configuration (config/filter_rules.yaml)

```yaml
# è‡ªåŠ¨ç­›é€‰è§„åˆ™é…ç½®
# è¿è¥å›¢é˜Ÿå¯ä»¥è°ƒæ•´è¿™äº›è§„åˆ™æ¥æ§åˆ¶ç”Ÿæˆçš„å†…å®¹ç±»å‹

# å„å¹³å°æœ€ä½çƒ­åº¦è¦æ±‚
engagement_thresholds:
  hackernews: 100      # HNè‡³å°‘100ä¸ªupvotes
  reddit: 500          # Redditè‡³å°‘500 karma
  newsapi: 5000        # æ–°é—»è‡³å°‘5000æ¬¡åˆ†äº«

# å¿…é¡»åŒ…å«çš„å…³é”®è¯ï¼ˆè‡³å°‘ä¸€ä¸ªï¼‰
topic_keywords:
  # æŠ€æœ¯å…³é”®è¯
  tech:
    - AI
    - GPT
    - OpenAI
    - äººå·¥æ™ºèƒ½
    - æœºå™¨å­¦ä¹ 
    - blockchain
    - åŒºå—é“¾
    - programming
    - ç¼–ç¨‹
    - software
    - è½¯ä»¶
    - cloud
    - äº‘è®¡ç®—
    - database
    - æ•°æ®åº“
    - security
    - å®‰å…¨
    - DevOps
    - frontend
    - backend
    - algorithm
    - ç®—æ³•

  # æ–°é—»å…³é”®è¯
  news:
    - breakthrough  # çªç ´
    - çªç ´
    - announces     # å®£å¸ƒ
    - å‘å¸ƒ
    - releases      # å‘å¸ƒ
    - launches      # æ¨å‡º
    - æ¨å‡º
    - acquires      # æ”¶è´­
    - æ”¶è´­
    - funding       # èèµ„
    - èèµ„
    - IPO
    - partnership   # åˆä½œ
    - åˆä½œ

# æ¯å¤©ç”Ÿæˆæ–‡ç« æ•°é‡ä¸Šé™
daily_limit: 10

# æ—¶æ•ˆæ€§è¦æ±‚
recency_hours: 24  # åªå¤„ç†24å°æ—¶å†…çš„trending

# å»é‡è®¾ç½®
deduplication:
  similarity_threshold: 0.85  # æ ‡é¢˜ç›¸ä¼¼åº¦è¶…è¿‡85%è§†ä¸ºé‡å¤
  check_days: 7              # æ£€æŸ¥æœ€è¿‘7å¤©æ˜¯å¦å·²ç”Ÿæˆè¿‡ç±»ä¼¼å†…å®¹
```

### 6. Main Pipeline (pipeline.py)

```python
#!/usr/bin/env python3
"""
TrendForge Pipeline - è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆç³»ç»Ÿ
"""

import asyncio
import argparse
from datetime import datetime
from pathlib import Path
import json
import yaml
import os
from typing import List, Dict

# Import all components
from crawlers.hackernews import HackerNewsCrawler
from crawlers.reddit import RedditCrawler
from crawlers.newsapi import NewsAPICrawler
from utils.filter import TrendingFilter
from utils.deduplicator import Deduplicator
from utils.storage import GitStorage
from generators.dr_generator import DRGenerator

class TrendForgePipeline:
    """ä¸»æµæ°´çº¿æ§åˆ¶å™¨"""

    def __init__(self):
        print("Initializing TrendForge Pipeline...")

        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self.crawlers = [
            HackerNewsCrawler(),
            RedditCrawler(),
            NewsAPICrawler()
        ]
        self.filter = TrendingFilter()
        self.deduplicator = Deduplicator()
        self.dr_generator = DRGenerator()
        self.storage = GitStorage()

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self._ensure_directories()

    def _ensure_directories(self):
        """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        directories = [
            'data/trending',
            'data/processed',
            'content/blog',
            'logs'
        ]
        for dir_path in directories:
            Path(dir_path).mkdir(parents=True, exist_ok=True)

    async def run_daily_pipeline(self):
        """
        æ‰§è¡Œæ¯æ—¥å†…å®¹ç”Ÿæˆæµç¨‹
        å®Œæ•´æµç¨‹ï¼šæŠ“å– â†’ å»é‡ â†’ ç­›é€‰ â†’ DRç”Ÿæˆ â†’ ä¿å­˜ â†’ Gitæäº¤
        """
        start_time = datetime.now()
        print(f"\n{'='*60}")
        print(f"TrendForge Daily Pipeline - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}\n")

        try:
            # Step 1: æŠ“å–trending
            print("ğŸ“¡ Step 1: Fetching trending topics from all sources...")
            all_trending = await self._fetch_all_trending()
            print(f"   âœ“ Fetched {len(all_trending)} total topics")

            # ä¿å­˜åŸå§‹trendingæ•°æ®
            self._save_raw_trending(all_trending)

            # Step 2: å»é‡
            print("\nğŸ” Step 2: Deduplicating topics...")
            unique_trending = self.deduplicator.deduplicate(all_trending)
            print(f"   âœ“ {len(unique_trending)} unique topics after deduplication")

            # Step 3: è‡ªåŠ¨ç­›é€‰
            print("\nğŸ¯ Step 3: Auto-filtering based on rules...")
            selected = self.filter.filter_trending(unique_trending)
            print(f"   âœ“ Selected {len(selected)} topics for generation")

            if not selected:
                print("   âš ï¸  No topics passed the filter criteria today")
                return

            # æ˜¾ç¤ºé€‰ä¸­çš„topics
            print("\n   Selected topics:")
            for i, topic in enumerate(selected, 1):
                print(f"   {i}. [{topic['source']}] {topic['title'][:60]}...")

            # ä¿å­˜ç­›é€‰åçš„æ•°æ®
            self._save_processed_trending(selected)

            # Step 4: ä½¿ç”¨DRç”Ÿæˆæ·±åº¦æ–‡ç« 
            print("\nğŸ“ Step 4: Generating deep research articles with MetaGPT DR...")
            articles = await self._generate_articles(selected)
            print(f"   âœ“ Successfully generated {len(articles)} articles")

            # Step 5: ä¿å­˜æ–‡ç« 
            print("\nğŸ’¾ Step 5: Saving articles to content/blog/...")
            saved_files = self._save_articles(articles)
            print(f"   âœ“ Saved {len(saved_files)} articles")

            # Step 6: Gitæäº¤
            print("\nğŸ“¤ Step 6: Committing to Git...")
            commit_message = f"feat: add {len(articles)} new articles - {datetime.now().strftime('%Y-%m-%d')}"
            self.storage.commit_and_push(commit_message)
            print("   âœ“ Committed and pushed to repository")

            # å®Œæˆ
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            print(f"\n{'='*60}")
            print(f"âœ… Pipeline completed successfully!")
            print(f"   Time taken: {duration:.1f} seconds")
            print(f"   Articles generated: {len(articles)}")
            print(f"{'='*60}\n")

        except Exception as e:
            print(f"\nâŒ Pipeline failed with error: {e}")
            raise

    async def _fetch_all_trending(self) -> List[Dict]:
        """å¹¶å‘ä»æ‰€æœ‰æºæŠ“å–trending"""
        tasks = []
        for crawler in self.crawlers:
            tasks.append(crawler.fetch_trending())

        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_items = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"   âš ï¸  Crawler {i} failed: {result}")
            else:
                all_items.extend(result)

        return all_items

    async def _generate_articles(self, topics: List[Dict]) -> List[tuple]:
        """
        ä½¿ç”¨DRç”Ÿæˆæ–‡ç« 
        é™åˆ¶å¹¶å‘æ•°é‡é¿å…APIè¿‡è½½
        """
        articles = []

        # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹3ä¸ªï¼ˆå¯é…ç½®ï¼‰
        batch_size = 3
        for i in range(0, len(topics), batch_size):
            batch = topics[i:i+batch_size]
            print(f"\n   Processing batch {i//batch_size + 1}...")

            tasks = []
            for topic in batch:
                tasks.append(self.dr_generator.generate_article(topic))

            results = await asyncio.gather(*tasks, return_exceptions=True)

            for topic, result in zip(batch, results):
                if isinstance(result, Exception):
                    print(f"   âš ï¸  Failed to generate article for: {topic['title'][:50]}")
                    print(f"      Error: {result}")
                else:
                    articles.append((topic, result))
                    print(f"   âœ“ Generated: {topic['title'][:50]}...")

        return articles

    def _save_articles(self, articles: List[tuple]) -> List[str]:
        """ä¿å­˜æ–‡ç« åˆ°content/blog/"""
        saved_files = []
        date_str = datetime.now().strftime('%Y-%m-%d')

        for topic, content in articles:
            # ç”Ÿæˆæ–‡ä»¶å
            slug = self._generate_slug(topic['title'])
            filename = f"{date_str}-{slug}.md"
            filepath = Path(f"content/blog/{filename}")

            # å†™å…¥æ–‡ä»¶
            filepath.write_text(content, encoding='utf-8')
            saved_files.append(str(filepath))

            print(f"   ğŸ’¾ {filename}")

        return saved_files

    def _generate_slug(self, title: str) -> str:
        """ç”ŸæˆURLå‹å¥½çš„æ–‡ä»¶å"""
        import re
        slug = re.sub(r'[^\w\s-]', '', title.lower())
        slug = re.sub(r'[-\s]+', '-', slug)
        return slug[:50]

    def _save_raw_trending(self, items: List[Dict]):
        """ä¿å­˜åŸå§‹trendingæ•°æ®ä¾›åˆ†æ"""
        date_str = datetime.now().strftime('%Y-%m-%d')
        filepath = Path(f"data/trending/{date_str}.json")

        data = {
            'date': date_str,
            'timestamp': datetime.now().isoformat(),
            'count': len(items),
            'items': items
        }

        filepath.write_text(json.dumps(data, ensure_ascii=False, indent=2, default=str))

    def _save_processed_trending(self, items: List[Dict]):
        """ä¿å­˜ç­›é€‰åçš„æ•°æ®"""
        date_str = datetime.now().strftime('%Y-%m-%d')
        filepath = Path(f"data/processed/{date_str}.json")

        data = {
            'date': date_str,
            'timestamp': datetime.now().isoformat(),
            'count': len(items),
            'items': items
        }

        filepath.write_text(json.dumps(data, ensure_ascii=False, indent=2, default=str))

def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(description='TrendForge Pipeline')
    parser.add_argument(
        'command',
        choices=['full', 'test', 'crawl'],
        help='Command to run'
    )

    args = parser.parse_args()

    # åˆå§‹åŒ–pipeline
    pipeline = TrendForgePipeline()

    if args.command == 'full':
        # è¿è¡Œå®Œæ•´pipeline
        asyncio.run(pipeline.run_daily_pipeline())

    elif args.command == 'crawl':
        # åªè¿è¡Œçˆ¬è™«æµ‹è¯•
        async def test_crawl():
            items = await pipeline._fetch_all_trending()
            print(f"Fetched {len(items)} items")
            for item in items[:5]:
                print(f"- [{item['source']}] {item['title'][:50]}...")

        asyncio.run(test_crawl())

    elif args.command == 'test':
        # è¿è¡Œæµ‹è¯•
        print("Running tests...")
        # TODO: Add test implementation

if __name__ == "__main__":
    main()
```

### 7. Git Storage Utils (utils/storage.py)

```python
from git import Repo
from pathlib import Path
import os

class GitStorage:
    """Gitæ“ä½œå·¥å…·ç±»"""

    def __init__(self, repo_path='.'):
        """åˆå§‹åŒ–Gitä»“åº“"""
        self.repo_path = Path(repo_path)

        try:
            self.repo = Repo(self.repo_path)
        except:
            print("Initializing git repository...")
            self.repo = Repo.init(self.repo_path)

    def commit_and_push(self, message: str):
        """æäº¤å¹¶æ¨é€åˆ°è¿œç¨‹ä»“åº“"""
        try:
            # æ·»åŠ contentç›®å½•çš„æ‰€æœ‰æ›´æ”¹
            self.repo.index.add(['content/'])

            # æäº¤
            self.repo.index.commit(message)
            print(f"   âœ“ Committed: {message}")

            # æ¨é€åˆ°è¿œç¨‹ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if self.repo.remotes:
                origin = self.repo.remote('origin')
                origin.push()
                print("   âœ“ Pushed to origin")
            else:
                print("   âš ï¸  No remote configured, skipping push")

        except Exception as e:
            print(f"   âš ï¸  Git operation failed: {e}")
```

### 8. Installation & Setup Script (setup.sh)

```bash
#!/bin/bash

echo "==================================="
echo "TrendForge Backend Setup"
echo "==================================="

# Create virtual environment
echo "Creating Python virtual environment..."
python3 -m venv venv
source venv/bin/activate

# Install dependencies
echo "Installing Python dependencies..."
pip install --upgrade pip
pip install -r backend/requirements.txt

# Install MetaGPT
echo "Installing MetaGPT DR module..."
pip install git+ssh://git@gitlab.deepwisdomai.com/pub/MetaGPT.git@dr4run

# Setup configuration
echo "Setting up configuration files..."
if [ ! -f backend/config/api_config.yaml ]; then
    cp backend/config/api_config.yaml.template backend/config/api_config.yaml
    echo "âš ï¸  Please edit backend/config/api_config.yaml with your API keys"
fi

# Setup MetaGPT config
echo "Setting up MetaGPT configuration..."
mkdir -p ~/.metagpt
if [ ! -f ~/.metagpt/config2.yaml ]; then
    echo "âš ï¸  Please copy your MetaGPT config to ~/.metagpt/config2.yaml"
fi

# Create necessary directories
echo "Creating project directories..."
mkdir -p data/trending data/processed content/blog logs

# Test installation
echo ""
echo "Testing installation..."
python -c "import metagpt; print('âœ“ MetaGPT installed')"
python -c "import aiohttp; print('âœ“ Dependencies installed')"

echo ""
echo "==================================="
echo "Setup complete!"
echo ""
echo "Next steps:"
echo "1. Edit backend/config/api_config.yaml with your API keys"
echo "2. Copy MetaGPT config to ~/.metagpt/config2.yaml"
echo "3. Run: python backend/pipeline.py full"
echo "==================================="
```

## Key Changes from Previous Version

1. **Removed blog_generator.py** - DRç›´æ¥ç”Ÿæˆæœ€ç»ˆå†…å®¹
2. **Simplified pipeline** - å»æ‰äº†DRâ†’Blogè½¬æ¢æ­¥éª¤
3. **Cost reduction** - ä¸éœ€è¦è°ƒç”¨GPT-4ï¼Œåªç”¨MetaGPT DR
4. **Better quality** - DRç”Ÿæˆçš„æ·±åº¦ç ”ç©¶æ–‡ç« è´¨é‡æ›´é«˜

## Testing Instructions

1. **Test DR Connection First**:
```bash
python -c "
from metagpt.environment.mgx.mgx_env import MGXEnv
from metagpt.roles.dr.research_leader import Researcher
researcher = Researcher()
researcher.rc.env = MGXEnv()
print('DR initialized successfully')
"
```

2. **Test Crawlers**:
```bash
python backend/pipeline.py crawl
```

3. **Run Full Pipeline**:
```bash
python backend/pipeline.py full
```

## Expected Daily Output

æ¯å¤©è‡ªåŠ¨ç”Ÿæˆ5-10ç¯‡æ·±åº¦æ–‡ç« ï¼š
```
content/blog/
â”œâ”€â”€ 2025-01-24-openai-releases-gpt5.md       (1200å­—æ·±åº¦åˆ†æ)
â”œâ”€â”€ 2025-01-24-nvidia-new-gpu-architecture.md (1500å­—æŠ€æœ¯è§£è¯»)
â”œâ”€â”€ 2025-01-24-google-quantum-breakthrough.md  (1000å­—è¡Œä¸šå½±å“)
â””â”€â”€ ...
```

æ¯ç¯‡æ–‡ç« åŒ…å«ï¼š
- æ·±åº¦æŠ€æœ¯åˆ†æ
- è¡Œä¸šå½±å“è¯„ä¼°
- æ•°æ®å’Œå¼•ç”¨
- æœªæ¥å±•æœ›
- ç›¸å…³é“¾æ¥

## Success Metrics

- [ ] æ¯å¤©è‡ªåŠ¨è¿è¡Œï¼Œæ— éœ€äººå·¥å¹²é¢„
- [ ] ç”Ÿæˆ5-10ç¯‡é«˜è´¨é‡æ·±åº¦æ–‡ç« 
- [ ] æ¯ç¯‡æ–‡ç« 1000-1500å­—
- [ ] è‡ªåŠ¨æ¨é€åˆ°Gitè§¦å‘ç½‘ç«™æ›´æ–°
- [ ] è¿è¥å¯ç›´æ¥åœ¨ç½‘ç«™æŸ¥çœ‹å’Œä½¿ç”¨
- [ ] æ€»æ‰§è¡Œæ—¶é—´ < 60åˆ†é’Ÿ